{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MSc in Data Science and Big Data \n","## Master Thesis\n","## Innovation & Entrepreneurship Business School\n","### Guillermo Altesor \n"]},{"cell_type":"markdown","metadata":{},"source":["In this Notebook we will do Sentiment Analysis of our TripAdvisor reviews using Transformers, specifically roBERTa."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8594,"status":"ok","timestamp":1662037343984,"user":{"displayName":"Guillermo Altesor","userId":"03779576127251618079"},"user_tz":-60},"id":"zkIt7_lKp0h-","outputId":"84be89bb-8229-4e3b-fd17-68b781a89965"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n","     ---------------------------------------- 4.9/4.9 MB 4.6 MB/s eta 0:00:00\n","Collecting filelock\n","  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n","Collecting huggingface-hub<1.0,>=0.9.0\n","  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n","     ------------------------------------- 163.5/163.5 kB 10.2 MB/s eta 0:00:00\n","Requirement already satisfied: requests in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.28.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp310-cp310-win_amd64.whl (3.3 MB)\n","     ---------------------------------------- 3.3/3.3 MB 9.1 MB/s eta 0:00:00\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (4.64.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n","     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2022.9.13)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (1.23.2)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: colorama in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2022.6.15.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (1.26.12)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2.1.1)\n","Installing collected packages: tokenizers, pyyaml, filelock, huggingface-hub, transformers\n","Successfully installed filelock-3.8.0 huggingface-hub-0.10.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.22.2\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\guill\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\guill\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"]}],"source":["# Install the transformers library#\n","!pip install transformers"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\guill\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\caffe2\\\\python\\\\serialized_test\\\\data\\\\operator_test\\\\piecewise_linear_transform_test.test_multi_predictions_params_from_arg.zip'\n","HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n","\n"]},{"name":"stdout","output_type":"stream","text":["Collecting torch\n","  Downloading torch-1.12.1-cp310-cp310-win_amd64.whl (162.2 MB)\n","     -------------------------------------- 162.2/162.2 MB 5.3 MB/s eta 0:00:00\n","Requirement already satisfied: typing-extensions in c:\\users\\guill\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.3.0)\n","Installing collected packages: torch\n"]}],"source":["# Install the torch library\n","!pip install torch"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"0wC0q6Bxp3or"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\guill\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# Import required packages\n","import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SZNal9hXp6wm"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 256/256 [00:00<00:00, 83.5kB/s]\n","C:\\Users\\guill\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\guill\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","Downloading: 100%|██████████| 687/687 [00:00<00:00, 222kB/s]\n","Downloading: 100%|██████████| 798k/798k [00:00<00:00, 1.09MB/s]\n","Downloading: 100%|██████████| 456k/456k [00:00<00:00, 955kB/s] \n","Downloading: 100%|██████████| 150/150 [00:00<00:00, 45.0kB/s]\n","Downloading: 100%|██████████| 1.42G/1.42G [06:01<00:00, 3.93MB/s]\n"]}],"source":["# Load tokenizer and model, create trainer\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NYZe1zzc1TiV"},"outputs":[],"source":["#!pip install drive"]},{"cell_type":"markdown","metadata":{},"source":["**To explore different options, we will use Google Drive, saving our CSV there and loading it from Drive to Colab**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2318,"status":"ok","timestamp":1662032327863,"user":{"displayName":"Guillermo Altesor","userId":"03779576127251618079"},"user_tz":-60},"id":"d8cEaNRjxzLX","outputId":"ed413333-641b-4e29-8f96-ed4590ec60a5"},"outputs":[],"source":["file_name = \"Cured_ATT.csv\" \n","text_column = \"Review\"\n","\n","df_pred = pd.read_csv(file_name)\n","pred_texts = df_pred[text_column].dropna().astype('str').tolist()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oKLUxGXmp7zF"},"outputs":[],"source":["# Tokenize texts and create prediction data set\n","tokenized_texts = tokenizer(pred_texts,truncation=True,padding=True)\n","pred_dataset = SimpleDataset(tokenized_texts)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"elapsed":928885,"status":"ok","timestamp":1662033259840,"user":{"displayName":"Guillermo Altesor","userId":"03779576127251618079"},"user_tz":-60},"id":"h5mjnob3sMCl","outputId":"7eb18b43-0204-44a3-aa55-86ac57af4753"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 9860\n","  Batch size = 8\n","  0%|          | 4/1233 [01:58<10:55:23, 32.00s/it]"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run predictions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_dataset\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2861\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2858\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2860\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2861\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2862\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[0;32m   2863\u001b[0m )\n\u001b[0;32m   2864\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2865\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2866\u001b[0m     speed_metrics(\n\u001b[0;32m   2867\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2871\u001b[0m     )\n\u001b[0;32m   2872\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2965\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2962\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[0;32m   2964\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 2965\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[0;32m   2966\u001b[0m inputs_decode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minclude_inputs_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2968\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:3218\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   3216\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3217\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3218\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   3219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n\u001b[0;32m   3220\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m outputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ignore_keys)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1210\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1207\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1210\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1211\u001b[0m     input_ids,\n\u001b[0;32m   1212\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1213\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1214\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1215\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1216\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1217\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1218\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1219\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1220\u001b[0m )\n\u001b[0;32m   1221\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1222\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    853\u001b[0m     embedding_output,\n\u001b[0;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    863\u001b[0m )\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:528\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    519\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    520\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    521\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    525\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    526\u001b[0m     )\n\u001b[0;32m    527\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 528\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    529\u001b[0m         hidden_states,\n\u001b[0;32m    530\u001b[0m         attention_mask,\n\u001b[0;32m    531\u001b[0m         layer_head_mask,\n\u001b[0;32m    532\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    533\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    534\u001b[0m         past_key_value,\n\u001b[0;32m    535\u001b[0m         output_attentions,\n\u001b[0;32m    536\u001b[0m     )\n\u001b[0;32m    538\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    539\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    402\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    403\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    411\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    414\u001b[0m         hidden_states,\n\u001b[0;32m    415\u001b[0m         attention_mask,\n\u001b[0;32m    416\u001b[0m         head_mask,\n\u001b[0;32m    417\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    418\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    419\u001b[0m     )\n\u001b[0;32m    420\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    422\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    331\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    332\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 340\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    341\u001b[0m         hidden_states,\n\u001b[0;32m    342\u001b[0m         attention_mask,\n\u001b[0;32m    343\u001b[0m         head_mask,\n\u001b[0;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    346\u001b[0m         past_key_value,\n\u001b[0;32m    347\u001b[0m         output_attentions,\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    350\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:266\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    263\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n\u001b[0;32m    265\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attention_scores, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    268\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:1834\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1833\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1834\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1836\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Run predictions\n","predictions = trainer.predict(pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3O53RHCsVd7"},"outputs":[],"source":["# Transform predictions to labels\n","preds = predictions.predictions.argmax(-1)\n","labels = pd.Series(preds).map(model.config.id2label)\n","scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1662033402244,"user":{"displayName":"Guillermo Altesor","userId":"03779576127251618079"},"user_tz":-60},"id":"FhIONI7ett0q","outputId":"3f08bca9-cfbc-408c-9869-d13dab96c4fc"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-68d90acd-4c1a-4d9c-8de9-70f2c17beca3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>pred</th>\n","      <th>label</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Worth the trip, cable car needs minimum 90 min...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998918</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Must see of Tenerife - A must see site on Tene...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998881</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A must visit place in tenerife. - Absolutely a...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998937</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Hike to the summit. - A drive up to El Tiede f...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998876</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Spectacular  - It's number one for a reason. O...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998924</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68d90acd-4c1a-4d9c-8de9-70f2c17beca3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-68d90acd-4c1a-4d9c-8de9-70f2c17beca3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-68d90acd-4c1a-4d9c-8de9-70f2c17beca3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                text  pred     label     score\n","0  Worth the trip, cable car needs minimum 90 min...     1  POSITIVE  0.998918\n","1  Must see of Tenerife - A must see site on Tene...     1  POSITIVE  0.998881\n","2  A must visit place in tenerife. - Absolutely a...     1  POSITIVE  0.998937\n","3  Hike to the summit. - A drive up to El Tiede f...     1  POSITIVE  0.998876\n","4  Spectacular  - It's number one for a reason. O...     1  POSITIVE  0.998924"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Create DataFrame with texts, predictions, labels, and scores\n","df = pd.DataFrame(list(zip(pred_texts,preds,labels,scores)), columns=['text','pred','label','score'])\n","df.head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1662033477641,"user":{"displayName":"Guillermo Altesor","userId":"03779576127251618079"},"user_tz":-60},"id":"DqFzQcTd6jW-","outputId":"92a64a99-d817-49ae-db93-24bb490ff4a0"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c48bd312-2aa2-424f-ac31-088ee3ea56cb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>pred</th>\n","      <th>label</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Worth the trip, cable car needs minimum 90 min...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998918</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Must see of Tenerife - A must see site on Tene...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998881</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A must visit place in tenerife. - Absolutely a...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998937</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Hike to the summit. - A drive up to El Tiede f...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998876</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Spectacular  - It's number one for a reason. O...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998924</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Stunning views - I was unable to complete the ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.992296</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Top of Spain - Clearly one of the best places ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998912</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Beautiful scenery - We hired a car to drive up...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998903</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Outstanding - Outstanding day and evening watc...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998940</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>A volcano - As part of this he Teide National ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998802</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Must see - We hired a car and head to El Teide...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998888</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Amazing experiences - Had a great time as we w...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998879</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Beautiful area. Lovely but demanding walks - W...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998756</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>A high spot.... - Its been many a long year si...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998878</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Incredible place!! - Absolutely amazing! The v...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998900</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Great! - Definitely a highlite of Tenerife.  G...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998686</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Really nicely preserved paths and the view is ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998852</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Mount Teide National Park - We had a fabulous ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998925</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>A \"must see \"on a visit to Tenerife - Having l...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998808</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Great Experience - We took a coach up with the...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998894</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Amazing! - With a superb view and a unique nat...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998925</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>spectacular views - great place to visit, but ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998871</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Tourist scam - Tickets are only sold online al...</td>\n","      <td>0</td>\n","      <td>NEGATIVE</td>\n","      <td>0.999506</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Highest peak in Spain - If you want to go with...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998577</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Amazing view! - The views from the top of the ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998854</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Worth a visit - Breath taking views from the t...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998941</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Amazing view of the whole island.  - Very nice...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998858</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Must visit when in Tenerife! - This place is i...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998927</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Top mountain! - El Teide is a must-visit if yo...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998785</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Must see - even you don't go to the top - Make...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998792</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Right Up There - Been before 25 years ago and ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998877</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>The must visit sight ! - Stunning. Easy to get...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998877</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>An absolute must. - A pure and simpel must see...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998880</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>Striking - The volcano is striking to see from...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998866</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>Amazing day  - An absolute must do, the scener...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998923</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>Breathtaking experience! - If u have good weat...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998699</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>Book before - We went to the volcano nov 8 th ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998689</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>Great scenery - Came here last week of October...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998884</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>Beautiful nature - Lovely envirement to experi...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998891</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>Teide tour - hope to see it again - If you vis...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998927</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>Don't do it - 2.5 hours sitting on a coach to ...</td>\n","      <td>0</td>\n","      <td>NEGATIVE</td>\n","      <td>0.999511</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>Such contrasts! - Loved going to the park here...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998916</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>Beautiful landscape  - I arranged half day tri...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998915</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>A unique place - Absolutely specific place. Be...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998855</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>Mount Teide is amazing - Mount Teide is a spec...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998873</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>Volcano Tiede Experience - A Must Do - Volcano...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998904</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>Teide Experience - The prices are pretty expen...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998836</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>Exceptional - This is another must if you visi...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998891</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>Beautiful  - A beautiful view from the top of ...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998932</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>Top views! - Hike to the peak of Teide was dif...</td>\n","      <td>1</td>\n","      <td>POSITIVE</td>\n","      <td>0.998879</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c48bd312-2aa2-424f-ac31-088ee3ea56cb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c48bd312-2aa2-424f-ac31-088ee3ea56cb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c48bd312-2aa2-424f-ac31-088ee3ea56cb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                 text  pred     label  \\\n","0   Worth the trip, cable car needs minimum 90 min...     1  POSITIVE   \n","1   Must see of Tenerife - A must see site on Tene...     1  POSITIVE   \n","2   A must visit place in tenerife. - Absolutely a...     1  POSITIVE   \n","3   Hike to the summit. - A drive up to El Tiede f...     1  POSITIVE   \n","4   Spectacular  - It's number one for a reason. O...     1  POSITIVE   \n","5   Stunning views - I was unable to complete the ...     1  POSITIVE   \n","6   Top of Spain - Clearly one of the best places ...     1  POSITIVE   \n","7   Beautiful scenery - We hired a car to drive up...     1  POSITIVE   \n","8   Outstanding - Outstanding day and evening watc...     1  POSITIVE   \n","9   A volcano - As part of this he Teide National ...     1  POSITIVE   \n","10  Must see - We hired a car and head to El Teide...     1  POSITIVE   \n","11  Amazing experiences - Had a great time as we w...     1  POSITIVE   \n","12  Beautiful area. Lovely but demanding walks - W...     1  POSITIVE   \n","13  A high spot.... - Its been many a long year si...     1  POSITIVE   \n","14  Incredible place!! - Absolutely amazing! The v...     1  POSITIVE   \n","15  Great! - Definitely a highlite of Tenerife.  G...     1  POSITIVE   \n","16  Really nicely preserved paths and the view is ...     1  POSITIVE   \n","17  Mount Teide National Park - We had a fabulous ...     1  POSITIVE   \n","18  A \"must see \"on a visit to Tenerife - Having l...     1  POSITIVE   \n","19  Great Experience - We took a coach up with the...     1  POSITIVE   \n","20  Amazing! - With a superb view and a unique nat...     1  POSITIVE   \n","21  spectacular views - great place to visit, but ...     1  POSITIVE   \n","22  Tourist scam - Tickets are only sold online al...     0  NEGATIVE   \n","23  Highest peak in Spain - If you want to go with...     1  POSITIVE   \n","24  Amazing view! - The views from the top of the ...     1  POSITIVE   \n","25  Worth a visit - Breath taking views from the t...     1  POSITIVE   \n","26  Amazing view of the whole island.  - Very nice...     1  POSITIVE   \n","27  Must visit when in Tenerife! - This place is i...     1  POSITIVE   \n","28  Top mountain! - El Teide is a must-visit if yo...     1  POSITIVE   \n","29  Must see - even you don't go to the top - Make...     1  POSITIVE   \n","30  Right Up There - Been before 25 years ago and ...     1  POSITIVE   \n","31  The must visit sight ! - Stunning. Easy to get...     1  POSITIVE   \n","32  An absolute must. - A pure and simpel must see...     1  POSITIVE   \n","33  Striking - The volcano is striking to see from...     1  POSITIVE   \n","34  Amazing day  - An absolute must do, the scener...     1  POSITIVE   \n","35  Breathtaking experience! - If u have good weat...     1  POSITIVE   \n","36  Book before - We went to the volcano nov 8 th ...     1  POSITIVE   \n","37  Great scenery - Came here last week of October...     1  POSITIVE   \n","38  Beautiful nature - Lovely envirement to experi...     1  POSITIVE   \n","39  Teide tour - hope to see it again - If you vis...     1  POSITIVE   \n","40  Don't do it - 2.5 hours sitting on a coach to ...     0  NEGATIVE   \n","41  Such contrasts! - Loved going to the park here...     1  POSITIVE   \n","42  Beautiful landscape  - I arranged half day tri...     1  POSITIVE   \n","43  A unique place - Absolutely specific place. Be...     1  POSITIVE   \n","44  Mount Teide is amazing - Mount Teide is a spec...     1  POSITIVE   \n","45  Volcano Tiede Experience - A Must Do - Volcano...     1  POSITIVE   \n","46  Teide Experience - The prices are pretty expen...     1  POSITIVE   \n","47  Exceptional - This is another must if you visi...     1  POSITIVE   \n","48  Beautiful  - A beautiful view from the top of ...     1  POSITIVE   \n","49  Top views! - Hike to the peak of Teide was dif...     1  POSITIVE   \n","\n","       score  \n","0   0.998918  \n","1   0.998881  \n","2   0.998937  \n","3   0.998876  \n","4   0.998924  \n","5   0.992296  \n","6   0.998912  \n","7   0.998903  \n","8   0.998940  \n","9   0.998802  \n","10  0.998888  \n","11  0.998879  \n","12  0.998756  \n","13  0.998878  \n","14  0.998900  \n","15  0.998686  \n","16  0.998852  \n","17  0.998925  \n","18  0.998808  \n","19  0.998894  \n","20  0.998925  \n","21  0.998871  \n","22  0.999506  \n","23  0.998577  \n","24  0.998854  \n","25  0.998941  \n","26  0.998858  \n","27  0.998927  \n","28  0.998785  \n","29  0.998792  \n","30  0.998877  \n","31  0.998877  \n","32  0.998880  \n","33  0.998866  \n","34  0.998923  \n","35  0.998699  \n","36  0.998689  \n","37  0.998884  \n","38  0.998891  \n","39  0.998927  \n","40  0.999511  \n","41  0.998916  \n","42  0.998915  \n","43  0.998855  \n","44  0.998873  \n","45  0.998904  \n","46  0.998836  \n","47  0.998891  \n","48  0.998932  \n","49  0.998879  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df.head(50)"]},{"cell_type":"markdown","metadata":{"id":"_Gt3Lri9ypyj"},"source":["@article{hartmann2022,\n","  title={More than a feeling: Accuracy and Application of Sentiment Analysis},\n","  author={Hartmann, Jochen and Heitmann, Mark and Siebert, Christian and Schamp, Christina},\n","  journal={International Journal of Research in Marketing},\n","  year={2022}\n","}"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"https://github.com/chrsiebert/sentiment-roberta-large-english/blob/main/sentiment_roberta_prediction_example.ipynb","timestamp":1662031295370}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.7 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"8bf6ae4cbc8e937395cc38608bb9f8ff38bcfe90be9f82d91b309898258bea19"}}},"nbformat":4,"nbformat_minor":0}
